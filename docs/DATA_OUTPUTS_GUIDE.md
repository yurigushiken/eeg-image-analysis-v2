# Scientific Data Outputs Guide

This guide documents the data files generated by the GFP-based ERP analysis pipeline for statistical analysis and publication.

## Overview

Each analysis generates **three types of output files**:

1. **Figures** (`.png`) - Publication-quality visualizations
2. **Collapsed Localizer Results** (`.json`) - Component-level GFP measurements
3. **Condition Measurements** (`.csv`) - Per-condition amplitude measurements for statistics

## File Locations

For each analysis (e.g., `cardinality_within_small`):

```
docs/assets/
├── plots/cardinality_within_small/
│   ├── cardinality_within_small-collapsed_localizer.png  # GFP traces
│   ├── cardinality_within_small-P1.png                   # P1 ERP + topomaps
│   └── cardinality_within_small-N1.png                   # N1 ERP + topomaps
│
└── tables/cardinality_within_small/
    ├── collapsed_localizer_results.json  # Component peaks and windows
    ├── condition_measurements.csv        # Amplitudes for statistical testing
    ├── qc_summary.csv                    # Subject inclusion/exclusion
    └── run_metrics.json                  # Pipeline performance metrics
```

## Data Files for Scientific Analysis

### 1. Collapsed Localizer Results (JSON)

**File**: `docs/assets/tables/{analysis_id}/collapsed_localizer_results.json`

**Purpose**: Documents the GFP-derived peak latencies and FWHM windows for each component. This provides full transparency about window selection.

**Structure**:
```json
{
  "analysis_id": "cardinality_within_small",
  "date_analyzed": "2025-10-12T02:27:38.323290",
  "baseline_ms": [-100, 0],
  "response_filter": "ALL",
  "n_subjects_total": 24,
  "n_evokeds_included": 72,
  "conditions": ["Cardinality1", "Cardinality2", "Cardinality3"],
  "components": {
    "P1": {
      "peak_latency_ms": 112.0,
      "peak_gfp_amplitude": 1.0030781841345152e-06,
      "fwhm_ms": 26.502012439441557,
      "window_start_ms": 98.28791551155996,
      "window_end_ms": 124.78992795100152,
      "search_range_ms": [60.0, 120.0],
      "method": "GFP_collapsed_localizer",
      "n_subjects": 72,
      "n_conditions": 3
    },
    "N1": {
      "peak_latency_ms": 180.0,
      "peak_gfp_amplitude": 1.5986746968202484e-06,
      "fwhm_ms": 58.03523515412667,
      "window_start_ms": 149.0109994548518,
      "window_end_ms": 207.04623460897847,
      "search_range_ms": [125.0, 200.0],
      "method": "GFP_collapsed_localizer",
      "n_subjects": 72,
      "n_conditions": 3
    }
  }
}
```

**Key Fields**:
- `peak_latency_ms`: GFP peak latency (same for all conditions within analysis)
- `peak_gfp_amplitude`: GFP amplitude at peak (in volts, not µV)
- `fwhm_ms`: Full Width at Half Maximum (data-driven window width)
- `window_start_ms` / `window_end_ms`: Exact time window boundaries
- `search_range_ms`: A priori search range from `configs/components.yaml`
- `method`: Always "GFP_collapsed_localizer" (for transparency)
- `n_subjects`: Total subject-condition combinations used (e.g., 24 subjects × 3 conditions = 72)

**Use Cases**:
- Report exact windows used in Methods section
- Document GFP peak latencies
- Verify consistency across analyses
- Compare FWHM widths between components

---

### 2. Condition Measurements (CSV)

**File**: `docs/assets/tables/{analysis_id}/condition_measurements.csv`

**Purpose**: Per-condition amplitude measurements for statistical analysis. Use this file to test hypotheses about amplitude differences between conditions.

**Structure**:
```csv
analysis_id,component,condition,n_subjects,peak_latency_ms,window_start_ms,window_end_ms,fwhm_ms,mean_amplitude_roi,roi_channels
cardinality_within_small,P1,Cardinality1,24,112.0,98.288,124.790,26.502,2.211e-06,E71;E75;E76;E70;E83;E74;E81;E82
cardinality_within_small,P1,Cardinality2,24,112.0,98.288,124.790,26.502,8.451e-07,E71;E75;E76;E70;E83;E74;E81;E82
cardinality_within_small,P1,Cardinality3,24,112.0,98.288,124.790,26.502,8.922e-07,E71;E75;E76;E70;E83;E74;E81;E82
cardinality_within_small,N1,Cardinality1,24,180.0,149.011,207.046,58.035,-9.766e-07,E66;E65;...
cardinality_within_small,N1,Cardinality2,24,180.0,149.011,207.046,58.035,-2.686e-06,E66;E65;...
cardinality_within_small,N1,Cardinality3,24,180.0,149.011,207.046,58.035,-2.753e-06,E66;E65;...
```

**Key Fields**:
- `analysis_id`: Which analysis generated this measurement
- `component`: ERP component (P1, N1, P3b)
- `condition`: Experimental condition name
- `n_subjects`: Number of subjects included
- `peak_latency_ms`: Peak latency (same across conditions - from collapsed localizer)
- `window_start_ms` / `window_end_ms`: Time window used for amplitude measurement
- `fwhm_ms`: Window width (Full Width at Half Maximum)
- `mean_amplitude_roi`: **Mean amplitude in ROI channels within FWHM window** (in volts)
- `roi_channels`: Semicolon-separated list of electrode labels

**Important Notes**:
- **Amplitudes are in volts, not microvolts** (e.g., 2.211e-06 = 2.211 µV)
- All conditions within an analysis use the **same peak latency and window**
- This prevents circular analysis (no "peeking" at individual conditions)
- `mean_amplitude_roi` is the **dependent variable** for statistical tests

---

## Statistical Analysis Examples

### Example 1: Test if N1 Amplitude Differs by Cardinality

**Hypothesis**: N1 amplitude increases with increasing cardinality.

**R Code**:
```r
library(tidyverse)

# Load data
data <- read_csv("docs/assets/tables/cardinality_within_small/condition_measurements.csv")

# Filter to N1 component and convert to µV
n1_data <- data %>%
  filter(component == "N1") %>%
  mutate(amplitude_uv = mean_amplitude_roi * 1e6)  # Convert V to µV

# View data
n1_data %>%
  select(condition, amplitude_uv)

# One-way ANOVA
model <- aov(amplitude_uv ~ condition, data = n1_data)
summary(model)

# Post-hoc comparisons
TukeyHSD(model)

# Effect size
library(effectsize)
eta_squared(model)
```

**Python Code**:
```python
import pandas as pd
import scipy.stats as stats

# Load data
data = pd.read_csv("docs/assets/tables/cardinality_within_small/condition_measurements.csv")

# Filter to N1 and convert to µV
n1_data = data[data['component'] == 'N1'].copy()
n1_data['amplitude_uv'] = n1_data['mean_amplitude_roi'] * 1e6

# Group by condition
card1 = n1_data[n1_data['condition'] == 'Cardinality1']['amplitude_uv']
card2 = n1_data[n1_data['condition'] == 'Cardinality2']['amplitude_uv']
card3 = n1_data[n1_data['condition'] == 'Cardinality3']['amplitude_uv']

# One-way ANOVA
f_stat, p_value = stats.f_oneway(card1, card2, card3)
print(f"F = {f_stat:.3f}, p = {p_value:.4f}")

# Pairwise t-tests with Bonferroni correction
from scipy.stats import ttest_ind
alpha = 0.05 / 3  # Bonferroni correction for 3 comparisons
t1, p1 = ttest_ind(card1, card2)
t2, p2 = ttest_ind(card2, card3)
t3, p3 = ttest_ind(card1, card3)

print(f"Card1 vs Card2: t = {t1:.3f}, p = {p1:.4f}")
print(f"Card2 vs Card3: t = {t2:.3f}, p = {p2:.4f}")
print(f"Card1 vs Card3: t = {t3:.3f}, p = {p3:.4f}")
```

---

### Example 2: Compare P1 Amplitude Between Two Conditions

**Hypothesis**: P1 amplitude differs between Small_Increasing and Small_Decreasing.

**R Code**:
```r
# Load data
data <- read_csv("docs/assets/tables/small_increasing_vs_decreasing/condition_measurements.csv")

# Filter to P1 and convert to µV
p1_data <- data %>%
  filter(component == "P1") %>%
  mutate(amplitude_uv = mean_amplitude_roi * 1e6)

# Paired t-test (if same subjects in both conditions)
t.test(amplitude_uv ~ condition, data = p1_data, paired = TRUE)

# Effect size (Cohen's d)
library(effsize)
cohen.d(amplitude_uv ~ condition, data = p1_data)

# Plot
library(ggplot2)
ggplot(p1_data, aes(x = condition, y = amplitude_uv)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.3) +
  labs(title = "P1 Amplitude by Condition",
       x = "Condition", y = "Amplitude (µV)") +
  theme_minimal()
```

---

### Example 3: Combine Multiple Analyses

**Goal**: Compare N1 latencies across different analyses to see if they're consistent.

**R Code**:
```r
library(tidyverse)

# Load collapsed localizer results from all analyses
analyses <- c("cardinality_within_small", "from1_to_any",
              "small_increasing_vs_decreasing", "small_increasing_vs_decreasing_ACC1")

# Function to extract N1 peak latency
extract_n1_latency <- function(analysis_id) {
  path <- sprintf("docs/assets/tables/%s/collapsed_localizer_results.json", analysis_id)
  data <- jsonlite::read_json(path)
  tibble(
    analysis = analysis_id,
    n1_peak_ms = data$components$N1$peak_latency_ms,
    n1_fwhm_ms = data$components$N1$fwhm_ms,
    n_subjects = data$n_subjects_total
  )
}

# Extract for all analyses
latencies <- map_df(analyses, extract_n1_latency)

# View results
latencies

# Summary statistics
latencies %>%
  summarise(
    mean_peak = mean(n1_peak_ms),
    sd_peak = sd(n1_peak_ms),
    mean_fwhm = mean(n1_fwhm_ms),
    sd_fwhm = sd(n1_fwhm_ms)
  )
```

---

## Publication Methods Template

Use the data files to report your analysis approach transparently:

### Methods Section Example

> **ERP Analysis.** We used a GFP-based collapsed localizer approach (Luck & Gaspelin, 2017) to identify component peaks and time windows in an unbiased manner. For each component (P1, N1), we computed the Global Field Power (GFP; Lehmann & Skrandies, 1980) - the spatial standard deviation across all 128 electrodes - by averaging across all experimental conditions. We identified component peaks within a priori search ranges (P1: 60-120 ms; N1: 125-200 ms) based on prior literature. Time window widths were determined by calculating the Full Width at Half Maximum (FWHM) of the GFP peak, yielding data-driven windows (P1: 26.5 ms; N1: 58.0 ms). We then applied these common peak latencies and time windows to all experimental conditions, ensuring no circular analysis (Kriegeskorte et al., 2009). Mean amplitudes were extracted from predefined ROI channels (P1: 8 centro-parietal electrodes; N1: 14 bilateral occipito-temporal electrodes) within the FWHM-derived time windows. All window parameters are available in the supplementary materials.

### Results Section Example

> **N1 Amplitude.** N1 amplitude (measured at 180 ms, time window: 149-207 ms) showed a significant main effect of cardinality, F(2, 46) = X.XX, p < .001, η² = 0.XX. Post-hoc comparisons revealed that N1 amplitude was more negative for Cardinality1 (M = -0.98 µV, SD = X.XX) compared to Cardinality2 (M = -2.69 µV, SD = X.XX), t(23) = X.XX, p < .001, d = X.XX, and Cardinality3 (M = -2.75 µV, SD = X.XX), t(23) = X.XX, p < .001, d = X.XX.

---

## Key Concepts

### Why All Conditions Use the Same Peak Latency

**This is scientifically correct!** The collapsed localizer approach:

1. **Prevents circular analysis**: We don't "peek" at individual conditions to find their peaks
2. **Reduces researcher degrees of freedom**: No post-hoc decisions about windows per condition
3. **Tests amplitude, not latency**: With this approach, you test **amplitude differences** between conditions

**What you CAN test**:
- ✅ Amplitude differences between conditions (primary use case)
- ✅ Scalp distribution differences
- ✅ Condition × Component interactions

**What you CANNOT test**:
- ❌ Peak latency differences between conditions (all use same peak from collapsed localizer)

If you want to test **latency differences**, you would need a different analysis approach (e.g., jackknife-based latency analysis).

---

### Understanding the Amplitude Values

**Important**: Amplitudes in the CSV are in **volts**, not microvolts!

- `2.211e-06` volts = `2.211` µV (positive)
- `-2.753e-06` volts = `-2.753` µV (negative)

**To convert to µV in your analysis**:
```python
data['amplitude_uv'] = data['mean_amplitude_roi'] * 1e6
```

```r
data <- data %>% mutate(amplitude_uv = mean_amplitude_roi * 1e6)
```

---

### ROI Channels

The `roi_channels` field documents exactly which electrodes were used for amplitude measurement:

- **P1**: `E71;E75;E76;E70;E83;E74;E81;E82` (8 centro-parietal electrodes)
- **N1**: `E66;E65;E59;E60;E67;E71;E70;E84;E76;E77;E85;E91;E90;E83` (14 bilateral occipito-temporal electrodes)

These are defined in `configs/electrodes.yaml` and should be reported in your Methods section.

---

## Quality Control

### QC Summary File

**File**: `docs/assets/tables/{analysis_id}/qc_summary.csv`

Documents which subjects were included/excluded and why:

```csv
subject,set,included,epoch_count,exclusion_reason
sub-01,Cardinality1,True,45,
sub-01,Cardinality2,True,42,
sub-02,Cardinality1,False,8,insufficient_epochs(<10)
```

**Use this to**:
- Report subject inclusion criteria
- Document data quality
- Ensure sufficient trial counts per condition

---

## Troubleshooting

### Issue: Amplitudes seem too small

**Solution**: They're in volts! Multiply by 1e6 to convert to µV.

### Issue: All conditions have the same peak latency

**Solution**: This is correct! The collapsed localizer uses one peak for all conditions.

### Issue: Need to test latency differences

**Solution**: The current pipeline tests amplitude differences only. For latency analysis, you would need a different approach (e.g., jackknife method) or could analyze individual condition GFP peaks separately (though this increases researcher degrees of freedom).

### Issue: Want to analyze at the subject level

**Solution**: The current pipeline provides grand average measurements. For subject-level statistics, you would need to modify `run_analysis.py` to save subject-level amplitudes instead of grand averages. Let me know if you need this!

---

## References

- Kriegeskorte, N., Simmons, W. K., Bellgowan, P. S., & Baker, C. I. (2009). Circular analysis in systems neuroscience: the dangers of double dipping. *Nature Neuroscience*, 12(5), 535-540.

- Lehmann, D., & Skrandies, W. (1980). Reference-free identification of components of checkerboard-evoked multichannel potential fields. *Electroencephalography and Clinical Neurophysiology*, 48(6), 609-621.

- Luck, S. J., & Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn't). *Psychophysiology*, 54(1), 146-157.

---

## Summary

**For routine statistical analysis**, you primarily need:

1. **`condition_measurements.csv`** - Contains amplitude measurements for statistical tests
2. **`collapsed_localizer_results.json`** - Documents the windows used for transparency

Load the CSV into R/Python/SPSS, convert amplitudes to µV, and run your statistical tests!

For questions or to request additional data outputs, please open an issue on GitHub.
